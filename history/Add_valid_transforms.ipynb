{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cjfghk5697/anomaly-detection-competition/blob/main/Add_valid_transforms.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IbKCV5mvcNt"
      },
      "source": [
        "# 기록\n",
        "\n",
        "## 07-04\n",
        "기본적인 transforms valid 추가. 바뀐점은 밑에 기록해놓는다.\n",
        "\n",
        "\n",
        "## transfors 추가\n",
        "```python\n",
        "class Custom_dataset(Dataset):\n",
        "    def __init__(self, \n",
        "                 img_paths, \n",
        "                 labels, \n",
        "                 mode='train',\n",
        "                 transforms= Sequence[Callable]\n",
        "            ) -> None:\n",
        "        self.img_paths = img_paths\n",
        "        self.labels = labels\n",
        "        self.mode=mode\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "    def __getitem__(self, idx):\n",
        "        img = self.img_paths[idx]\n",
        "        if self.mode=='train':\n",
        "            augmentation = random.randint(0,2)\n",
        "            if augmentation==1:\n",
        "                img = img[::-1].copy()\n",
        "            elif augmentation==2:\n",
        "                img = img[:,::-1].copy()\n",
        "        img = transforms.ToTensor()(img)\n",
        "        if self.mode=='test':\n",
        "            pass\n",
        "        if self.transforms is not None:\n",
        "            img = self.transforms(img)        \n",
        "        label = self.labels[idx]\n",
        "        return img, label\n",
        "    \n",
        "class Network(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Network, self).__init__()\n",
        "        self.model = timm.create_model('efficientnet_b0', pretrained=True, num_classes=88)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        return x\n",
        "```\n",
        "\n",
        "```python\n",
        "transforms_train = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomVerticalFlip(p=0.5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        [0.485, 0.456, 0.406],\n",
        "        [0.229, 0.224, 0.225]\n",
        "    )\n",
        "])\n",
        "```\n",
        "## valid 추가\n",
        "```python\n",
        "batch_size = 32\n",
        "epochs = 25\n",
        "\n",
        "# Train\n",
        "trainset  = Custom_dataset(np.array(train_imgs), np.array(train_labels), mode='train', transforms=transforms_train)\n",
        "lengths = [int(len(trainset)*0.8), int(len(trainset)*0.2)]\n",
        "\n",
        "\n",
        "train_dataset, valid_dataset = torch.utils.data.random_split(trainset, lengths)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
        "valid_loader = DataLoader(valid_dataset, shuffle=True, batch_size=batch_size)\n",
        "```\n",
        "\n",
        "```python\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      for i, (images, targets) in enumerate(valid_loader):\n",
        "\n",
        "          images = images.to(device)\n",
        "          targets = targets.to(device)\n",
        "\n",
        "          outputs = model(images)\n",
        "          valid_loss = criterion(outputs, targets)\n",
        "      \n",
        "          outputs = outputs > 0.5\n",
        "          valid_acc = (outputs == targets).cpu().float().mean()\n",
        "          valid_acc_list.append(valid_acc)\n",
        "          valid_loss_list.append(valid_loss)\n",
        "          print(f'{epoch}: \"val loss:\"{valid_loss.item():.5f}, \"val acc: \"{valid_acc.item():.5f}')        \n",
        "    \n",
        "    train_f1 = score_function(train_y, train_pred)\n",
        "\n",
        "    TIME = time.time() - start\n",
        "    print(f'epoch : {epoch+1}/{epochs}    time : {TIME:.0f}s/{TIME*(epochs-epoch-1):.0f}s')\n",
        "    print(f'TRAIN    loss : {train_loss:.5f}    f1 : {train_f1:.5f}')\n",
        "    print(f'VALID    loss:\"{valid_loss.item():.5f},    acc : \"{valid_acc.item():.5f}')\n",
        "    torch.save(model.state_dict(), f'{train_loss}_{train_f1}.pth')\n",
        "```\n",
        "\n",
        "## 뜬 error\n",
        "```python\n",
        "ValueError: Sum of input lengths does not equal the length of the input dataset!\n",
        "```\n",
        "해결법은 valid set으로 나눌때 length 배열을 보면 조금 부족하다.\n",
        "원래 총 길이는 4277인데 lengths의 합은 4276이다. 왜냐하면 아마도 파이썬 특성중 하나인 완벽하게 정수를 반환하지 않는다. 원래 완벽한 계산을 안하는 특성이 있던걸로 기억난다. 그래서 이런 결과가 나온거같다. 간단하게 lengths[1]에 1을 더해 해결했다.\n",
        "\n",
        "\n",
        "# 이런 작업은 이제 ez하다\n"
      ],
      "id": "9IbKCV5mvcNt"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nv5znRz5h3V"
      },
      "source": [
        "### 라이브러리 불러오기"
      ],
      "id": "6nv5znRz5h3V"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZ-sBHD67ara"
      },
      "source": [
        "https://dacon.io/competitions/official/235894/overview/description"
      ],
      "id": "CZ-sBHD67ara"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PzCathkdclQi",
        "outputId": "1fbb6789-3964-4186-c09e-d9708755fa35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "PzCathkdclQi"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XT1TIhwClLco",
        "outputId": "7377a5c4-79c9-405b-8fe8-a1ab7911c6a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/input\n"
          ]
        }
      ],
      "source": [
        "%cd \"/content/drive/MyDrive/input/\"\n",
        "!unzip -q \"/content/drive/MyDrive/input/train.zip\" "
      ],
      "id": "XT1TIhwClLco"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFbYUT85c21S",
        "outputId": "d741c042-ec90-40c2-ecb4-964a6bc4309b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting timm\n",
            "  Downloading timm-0.5.4-py3-none-any.whl (431 kB)\n",
            "\u001b[K     |████████████████████████████████| 431 kB 5.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.11.0+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.12.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (4.1.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (1.21.6)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (3.0.4)\n",
            "Installing collected packages: timm\n",
            "Successfully installed timm-0.5.4\n"
          ]
        }
      ],
      "source": [
        "!pip3 install timm"
      ],
      "id": "oFbYUT85c21S"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "QAYn-e555h3X"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from glob import glob\n",
        "import pandas as pd\n",
        "import numpy as np \n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "\n",
        "import os\n",
        "import timm\n",
        "import random\n",
        "from typing import Tuple, Sequence, Callable\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "import time\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "\n",
        "device = torch.device('cuda')"
      ],
      "id": "QAYn-e555h3X"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "4hqIke8a5h3Y"
      },
      "outputs": [],
      "source": [
        "train_png = sorted(glob('./train/*.png'))"
      ],
      "id": "4hqIke8a5h3Y"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "QqVmNfwZ5h3Z"
      },
      "outputs": [],
      "source": [
        "train_y = pd.read_csv('./train_df.csv')\n",
        "\n",
        "train_labels = train_y[\"label\"]\n",
        "\n",
        "label_unique = sorted(np.unique(train_labels))\n",
        "label_unique = {key:value for key,value in zip(label_unique, range(len(label_unique)))}\n",
        "\n",
        "train_labels = [label_unique[k] for k in train_labels]"
      ],
      "id": "QqVmNfwZ5h3Z"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "qo8oWVcz5h3Z"
      },
      "outputs": [],
      "source": [
        "def img_load(path):\n",
        "    img = cv2.imread(path)[:,:,::-1]\n",
        "    img = cv2.resize(img, (512, 512))\n",
        "    return img"
      ],
      "id": "qo8oWVcz5h3Z"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErXkcD7d5h3a",
        "outputId": "33f5905e-a06e-46d4-d5e0-c3fec075eac8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4277/4277 [02:54<00:00, 24.44it/s]\n"
          ]
        }
      ],
      "source": [
        "train_imgs = [img_load(m) for m in tqdm(train_png)]"
      ],
      "id": "ErXkcD7d5h3a"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "aQfAK_29vky9"
      },
      "outputs": [],
      "source": [
        "transforms_train = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomVerticalFlip(p=0.5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        [0.485, 0.456, 0.406],\n",
        "        [0.229, 0.224, 0.225]\n",
        "    )\n",
        "])\n"
      ],
      "id": "aQfAK_29vky9"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "GVB5_Crz5h3a"
      },
      "outputs": [],
      "source": [
        "class Custom_dataset(Dataset):\n",
        "    def __init__(self, \n",
        "                 img_paths, \n",
        "                 labels, \n",
        "                 mode='train',\n",
        "                 transforms= Sequence[Callable]\n",
        "            ) -> None:\n",
        "        self.img_paths = img_paths\n",
        "        self.labels = labels\n",
        "        self.mode=mode\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "    def __getitem__(self, idx):\n",
        "        img = self.img_paths[idx]\n",
        "        if self.mode=='train':\n",
        "            augmentation = random.randint(0,2)\n",
        "            if augmentation==1:\n",
        "                img = img[::-1].copy()\n",
        "            elif augmentation==2:\n",
        "                img = img[:,::-1].copy()\n",
        "        if self.mode=='test':\n",
        "            pass\n",
        "        img = Image.fromarray(img) # NumPy array to PIL image\n",
        "        if self.transforms is not None:\n",
        "            img = self.transforms(img)        \n",
        "        label = self.labels[idx]\n",
        "        return img, label\n",
        "    \n",
        "class Network(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Network, self).__init__()\n",
        "        self.model = timm.create_model('efficientnet_b0', pretrained=True, num_classes=88)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        return x"
      ],
      "id": "GVB5_Crz5h3a"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "cbhpWtL45h3b"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "epochs = 25\n",
        "\n",
        "# Train\n",
        "trainset  = Custom_dataset(np.array(train_imgs), np.array(train_labels), mode='train', transforms=transforms_train)\n",
        "lengths = [int(len(trainset)*0.8), int(len(trainset)*0.2)]\n"
      ],
      "id": "cbhpWtL45h3b"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "sdORtwaH4dpb"
      },
      "outputs": [],
      "source": [
        "\n",
        "lengths=[lengths[0],lengths[1]+1]"
      ],
      "id": "sdORtwaH4dpb"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "jzqumLFE4ATy"
      },
      "outputs": [],
      "source": [
        "train_dataset, valid_dataset = torch.utils.data.random_split(trainset, lengths)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
        "valid_loader = DataLoader(valid_dataset, shuffle=True, batch_size=batch_size)"
      ],
      "id": "jzqumLFE4ATy"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvqyW4n65h3b"
      },
      "source": [
        "### 모델 학습"
      ],
      "id": "hvqyW4n65h3b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QplKmNaD5h3b",
        "outputId": "5ea97c4c-93a3-4bdd-b868-f81bf84266de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch : 1/25    time : 82s/1961s\n",
            "TRAIN    loss : 1.10204    f1 : 0.19370\n",
            "valid    loss : 0.38223    f1 : 0.77778\n",
            "epoch : 2/25    time : 80s/1839s\n",
            "TRAIN    loss : 0.62112    f1 : 0.30558\n",
            "valid    loss : 0.60512    f1 : 0.59216\n"
          ]
        }
      ],
      "source": [
        "def score_function(real, pred):\n",
        "    score = f1_score(real, pred, average=\"macro\")\n",
        "    return score\n",
        "\n",
        "model = Network().to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "scaler = torch.cuda.amp.GradScaler() \n",
        "\n",
        "\n",
        "valid_loss_list=[]\n",
        "train_loss_list=[]\n",
        "\n",
        "best=0\n",
        "for epoch in range(epochs):\n",
        "    start=time.time()\n",
        "    train_loss = 0\n",
        "    train_pred=[]\n",
        "    train_y=[]\n",
        "\n",
        "    valid_loss = 0\n",
        "    valid_pred=[]\n",
        "    valid_y=[]\n",
        "\n",
        "    model.train()\n",
        "    for batch in (train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        x = torch.tensor(batch[0], dtype=torch.float32, device=device)\n",
        "        y = torch.tensor(batch[1], dtype=torch.long, device=device)\n",
        "        with torch.cuda.amp.autocast():\n",
        "            pred = model(x)\n",
        "        loss = criterion(pred, y)\n",
        "\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        \n",
        "        train_loss += loss.item()/len(train_loader)\n",
        "        train_pred += pred.argmax(1).detach().cpu().numpy().tolist()\n",
        "        train_y += y.detach().cpu().numpy().tolist()\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      for i in (valid_loader):\n",
        "          images = torch.tensor(batch[0], dtype=torch.float32, device=device)\n",
        "          targets = torch.tensor(batch[1], dtype=torch.long, device=device)\n",
        "\n",
        "\n",
        "\n",
        "          outputs = model(images)\n",
        "          loss = criterion(outputs, targets)\n",
        "\n",
        "          valid_loss += loss.item()/len(valid_loader)\n",
        "          valid_pred += outputs.argmax(1).detach().cpu().numpy().tolist()\n",
        "          valid_y += targets.detach().cpu().numpy().tolist()\n",
        "    \n",
        "    train_f1 = score_function(train_y, train_pred)\n",
        "    valid_f1 = score_function(valid_y, valid_pred)\n",
        "\n",
        "    TIME = time.time() - start\n",
        "    print(f'epoch : {epoch+1}/{epochs}    time : {TIME:.0f}s/{TIME*(epochs-epoch-1):.0f}s')\n",
        "    print(f'TRAIN    loss : {train_loss:.5f}    f1 : {train_f1:.5f}')\n",
        "    print(f'valid    loss : {valid_loss:.5f}    f1 : {valid_f1:.5f}')\n",
        "\n",
        "    torch.save(model.state_dict(), f'{train_loss}_{train_f1}.pth')"
      ],
      "id": "QplKmNaD5h3b"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Add valid, transforms.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}